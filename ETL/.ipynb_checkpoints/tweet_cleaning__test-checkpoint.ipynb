{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74181aac",
   "metadata": {},
   "source": [
    "# 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515569a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer= TreebankWordTokenizer()\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "skl_stopwords=_stop_words.ENGLISH_STOP_WORDS\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00123f15",
   "metadata": {},
   "source": [
    "# 2. Initializing MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d37a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## MONGODB\n",
    "HOST_MDB = 'mymongo' # if in docker it would be the container name\n",
    "PORT_MDB = 27017\n",
    "# Connection string\n",
    "conn_string_mdb = f\"mongodb://{HOST_MDB}:{PORT_MDB}\" \n",
    "client = MongoClient(conn_string_mdb)\n",
    "\n",
    "## POSTGRES\n",
    "USERNAME_PG = 'postgres'\n",
    "PASSWORD_PG = 'postgres'\n",
    "HOST_PG = '' # if in docker it would be the container name\n",
    "PORT_PG = 5432\n",
    "DATABASE_NAME_PG = 'posty_tweets'\n",
    "\n",
    "# Connection string\n",
    "conn_string_pg = f\"postgresql://{USERNAME_PG}:{PASSWORD_PG}@{HOST_PG}:{PORT_PG}/{DATABASE_NAME_PG}\" \n",
    "pg = create_engine(conn_string_pg,client_encoding='utf8')\n",
    "\n",
    "pg.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS song_table (\n",
    "    neg numeric,\n",
    "    new numeric,\n",
    "    pos numeric,\n",
    "    compound numeric,\n",
    "    tweet TEXT,\n",
    "    Sentiment_label TEXT\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7ccfc0",
   "metadata": {},
   "source": [
    "# 3. Calling database and Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a4d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract():\n",
    "    mydb = client[\"tweetcollector\"]\n",
    "    mycol = mydb[\"test\"] #vaxdata\n",
    "    ## 3.1 Accessing all the tweets\n",
    "    tweets = list(mycol.find())\n",
    "    ## 3.2 Converting the data to pandas dataframe\n",
    "    tweets_df = pd.DataFrame(data = tweets)\n",
    "    #tweets_df.head()\n",
    "    return(tweets_df)\n",
    "\n",
    "# 4 Text Cleaning and Processing\n",
    "\n",
    "def text_cleaning(text, stopwords=skl_stopwords):\n",
    "    panc = string.punctuation + '–'+ '‘'+ '’'+ '“'+'”'\n",
    "    text = [i for i in text if not re.findall(\"[^\\u0000-\\u05C0\\u2100-\\u214F]+\",i)]\n",
    "    text = ''.join([ch for ch in text if ch not in panc]) #remove punctuation\n",
    "    text = re.sub(pattern= '[0-9]+', string= text, repl = ' ' )\n",
    "    text = re.sub(pattern= '\\s', string= text, repl = ' ' )\n",
    "    text = re.sub(pattern = '(aah|aaaa|aa)', string = text, repl ='')\n",
    "    text = re.sub(pattern = '\\#\\S+', string = text, repl ='') # removes hastags\n",
    "    text = re.sub(pattern = '(\\#|@|http\\S+|[0-9]|\"|)',string = text, repl ='') #removes url, #,@,space, numbers\n",
    "    return text \n",
    "\n",
    "def transform(tweets_df):\n",
    "    #converting extacted tweet to list\n",
    "    sample_text = tweets_df.tweet\n",
    "    sample_lst = sample_text.tolist()\n",
    "    \n",
    "    #removing some come words\n",
    "    sample_lst\n",
    "    text_clean = []\n",
    "    for i in sample_lst:\n",
    "        prohibitedWords = ['Antivax','antivax','People','Covid','people','covid']\n",
    "        text = re.compile('|'.join(map(re.escape, prohibitedWords)))\n",
    "        text_clean.append(text.sub(\"\",i))\n",
    "    \n",
    "    \n",
    "    s = SentimentIntensityAnalyzer() #initializing the model\n",
    "    \n",
    "    sentiment_df = pd.DataFrame() #initializin the dataframe to store the sentiments\n",
    "    \n",
    "    for i,lst in enumerate(text_clean):#sample_lst\n",
    "        text = text_cleaning(lst,stopwords=skl_stopwords) #textcleaning\n",
    "        score = s.polarity_scores(text) #calculating sentiment\n",
    "        score['tweet'] = text #updating the output of sentiment analysis with the respective tweet\n",
    "        sentiment_df = sentiment_df.append(score,ignore_index = True) # updating the dataframe with results (Sentiments)\n",
    "    sentiment_df.reindex(columns=['tweet','neg','neu','pos','compound']) #reindexing the column \n",
    "    \n",
    "    # labeling sentiments\n",
    "    Sentiment_label = []\n",
    "    for i, row in sentiment_df.iterrows():\n",
    "        if (row['compound'] > 0.5):\n",
    "            Sentiment_label.append('Positive')\n",
    "        elif (row['compound'] >= -0.5 and row['compound'] <= 0.5):\n",
    "            Sentiment_label.append('Neutral')\n",
    "        else:\n",
    "            Sentiment_label.append('Negative')\n",
    "    \n",
    "    sentiment_df['Sentiment_label'] = Sentiment_label\n",
    "    #sns.countplot(data = sentiment_df, x  =sentiment_df['Sentiment_label'])\n",
    "    \n",
    "    \n",
    "    labels = ['Positive','Negative','Neutral']#'Highly Positive','Highly Negative'\n",
    "    for i, label in enumerate(labels):\n",
    "        i = i+1\n",
    "        print('-'*110)\n",
    "        print(i,'.','#'*30,label,'#'*30)\n",
    "        print('-'*110)\n",
    "        senti_labels = sentiment_df[sentiment_df['Sentiment_label'] == label]\n",
    "        #print(sneti_labels)\n",
    "        wordCloud = WordCloud(\n",
    "                background_color=\"black\", \n",
    "                width=1600, \n",
    "                height=800,\n",
    "                stopwords=skl_stopwords).generate(' '.join(senti_labels.tweet))\n",
    "\n",
    "        plt.figure(figsize=(15,10), facecolor='k')\n",
    "        plt.imshow(wordCloud,interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        return plt.show()\n",
    "    logging.critical(\"\\n---TRANSFORMATION COMPLETED---\")\n",
    "\n",
    "def load(sentiments):\n",
    "    sentiment_df.to_sql(pg, if_exists='replace')\n",
    "    logging.critical(f\"Sentiment loaded into postgres\")\n",
    "    \n",
    "    \n",
    "    \n",
    "TWEET = extract()\n",
    "sentiment_df = transform(tweets_df)\n",
    "load(sentiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1600e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca9dd17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22639128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
